{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779043d1",
   "metadata": {},
   "source": [
    "# Task 4: Conceptual Questions\n",
    "\n",
    "Answer briefly:\n",
    "\n",
    "# 1. What is entropy and information gain?\n",
    "\n",
    "Entropy is a measure of impurity or randomness in a dataset; it quantifies the uncertainty in predicting the class label. \n",
    "\n",
    "Information gain measures the reduction in entropy after splitting a dataset based on a feature, helping to select the feature that best separates the classes.\n",
    "\n",
    "# 2. Explain the difference between Gini Index and Entropy.\n",
    "\n",
    "\n",
    "Entropy:\n",
    "Both Gini Index and Entropy measure the impurity of a node in a decision tree. Entropy uses a logarithmic formula and can be more sensitive to changes in class distribution, while Gini Index is simpler (based on squared probabilities) and often faster to compute. Both are used to decide splits, but Gini tends to favor larger partitions.\n",
    "\n",
    "# 3. How can a decision tree overfit? How can this be avoided?\n",
    "\n",
    "A decision tree can overfit by growing too deep and capturing noise or minor fluctuations in the training data. Overfitting can be avoided by pruning the tree, setting a maximum depth, requiring a minimum number of samples per leaf, or using ensemble methods like Random Forests."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
