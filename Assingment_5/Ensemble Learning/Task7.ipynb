{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a1fdb1",
   "metadata": {},
   "source": [
    "\n",
    "# Task 7: Conceptual Questions\n",
    "\n",
    "Answer:\n",
    "\n",
    "# 1. What is the difference between Bagging and Boosting?\n",
    "\n",
    "Bagging builds multiple models independently using random subsets of data and combines their predictions, mainly to reduce variance.\n",
    "\n",
    "Boosting builds models sequentially, where each new model focuses on correcting the errors of the previous ones, aiming to reduce bias.\n",
    "\n",
    "Bagging is less prone to overfitting and works well with high-variance models like decision trees.\n",
    "\n",
    "Boosting can achieve higher accuracy but is more sensitive to noise and overfitting.\n",
    "\n",
    "\n",
    "# 2. How does Random Forest reduce variance?\n",
    "\n",
    "Random Forest reduces variance by averaging the predictions of many decision trees, each trained on different random subsets of the data and features. This ensemble approach ensures that the errors made by individual trees are averaged out, making the overall model less likely to overfit and more robust\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. What is the weakness of boosting-base methods?\n",
    "\n",
    "Boosting is sensitive to noisy data and outliers since it puts more focus on misclassified samples, which can hurt performance. It may overfit on small or noisy datasets, especially when too many weak learners are used. The method is also computationally intensive due to its sequential training process. Additionally, the final boosted model becomes complex and less interpretable. Without proper regularization, it can lead to overly complex models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
